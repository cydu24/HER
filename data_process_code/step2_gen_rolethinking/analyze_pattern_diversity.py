#!/usr/bin/env python3
"""
åˆ†æå¢å¼ºåæ•°æ®çš„ Pattern å¤šæ ·æ€§
ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå’Œ Markdown è¡¨æ ¼
"""
import json
import re
import math
from collections import Counter
from datetime import datetime
import argparse


def extract_pattern(content: str) -> str:
    """ä»å¢å¼ºå†…å®¹ä¸­æå– pattern"""
    if not content:
        return 'empty'
    
    segments = []
    for match in re.finditer(r'<role_thinking>.*?</role_thinking>', content, re.DOTALL):
        segments.append(('think', match.start(), match.end()))
    for match in re.finditer(r'<role_action>.*?</role_action>', content, re.DOTALL):
        segments.append(('act', match.start(), match.end()))
    
    segments.sort(key=lambda x: x[1])
    
    pattern = []
    last_end = 0
    
    for tag_type, start, end in segments:
        text_before = content[last_end:start].strip()
        text_before = re.sub(r'<role_thinking>.*?</role_thinking>', '', text_before, flags=re.DOTALL)
        text_before = re.sub(r'<role_action>.*?</role_action>', '', text_before, flags=re.DOTALL)
        if text_before and len(text_before) > 0:
            pattern.append('speech')
        pattern.append(tag_type)
        last_end = end
    
    text_after = content[last_end:].strip()
    if text_after and len(text_after) > 0:
        pattern.append('speech')
    
    return 'â†’'.join(pattern) if pattern else 'empty'


def calculate_entropy(counter: Counter, total: int) -> float:
    """è®¡ç®—é¦™å†œç†µ"""
    entropy = 0
    for count in counter.values():
        p = count / total
        if p > 0:
            entropy -= p * math.log2(p)
    return entropy


def analyze_data(input_path: str):
    """åˆ†ææ•°æ®å¹¶è¿”å›ç»Ÿè®¡ç»“æœ"""
    original_patterns = Counter()
    enhanced_patterns = Counter()
    patterns_per_sample = []
    
    total_samples = 0
    total_dialogues = 0
    
    with open(input_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            total_samples += 1
            
            sample_patterns = set()
            
            # åŸå§‹å¯¹è¯
            for d in data.get('original_dialogues', []):
                content = d.get('standard_format', '')
                pattern = extract_pattern(content)
                original_patterns[pattern] += 1
            
            # å¢å¼ºåå¯¹è¯
            for d in data.get('enhanced_dialogues', []):
                content = d.get('enhanced_role_think', '')
                pattern = extract_pattern(content)
                enhanced_patterns[pattern] += 1
                sample_patterns.add(pattern)
                total_dialogues += 1
            
            patterns_per_sample.append(len(sample_patterns))
    
    return {
        'total_samples': total_samples,
        'total_dialogues': total_dialogues,
        'original_patterns': original_patterns,
        'enhanced_patterns': enhanced_patterns,
        'patterns_per_sample': patterns_per_sample
    }


def generate_markdown_report(stats: dict, output_path: str):
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
    total_samples = stats['total_samples']
    total_dialogues = stats['total_dialogues']
    original_patterns = stats['original_patterns']
    enhanced_patterns = stats['enhanced_patterns']
    patterns_per_sample = stats['patterns_per_sample']
    
    orig_total = sum(original_patterns.values())
    orig_entropy = calculate_entropy(original_patterns, orig_total)
    enh_entropy = calculate_entropy(enhanced_patterns, total_dialogues)
    
    avg_patterns = sum(patterns_per_sample) / len(patterns_per_sample)
    
    # åˆ†ç»„
    high_freq = [(p, c, c/total_dialogues*100) for p, c in enhanced_patterns.most_common() if c/total_dialogues*100 > 1]
    medium_freq = [(p, c, c/total_dialogues*100) for p, c in enhanced_patterns.most_common() if 0.1 <= c/total_dialogues*100 <= 1]
    low_freq = [(p, c, c/total_dialogues*100) for p, c in enhanced_patterns.most_common() if c/total_dialogues*100 < 0.1]
    
    md = f"""# Pattern å¤šæ ·æ€§åˆ†ææŠ¥å‘Š

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š æ¦‚è§ˆ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ€»æ ·æœ¬æ•° | {total_samples:,} |
| æ€»å¯¹è¯æ•° | {total_dialogues:,} |
| å¹³å‡æ¯æ ·æœ¬å¯¹è¯æ•° | {total_dialogues/total_samples:.1f} |
| Pattern ç§ç±»æ•° | {len(enhanced_patterns)} |
| æ¯æ ·æœ¬å¹³å‡ Pattern ç§ç±» | {avg_patterns:.2f} |

## ğŸ“ˆ å¤šæ ·æ€§æŒ‡æ ‡

| æŒ‡æ ‡ | åŸå§‹ | å¢å¼ºå | å˜åŒ– |
|------|------|--------|------|
| Pattern ç§ç±»æ•° | {len(original_patterns)} | {len(enhanced_patterns)} | +{len(enhanced_patterns)-len(original_patterns)} |
| é¦™å†œç†µ | {orig_entropy:.4f} | {enh_entropy:.4f} | +{enh_entropy-orig_entropy:.4f} ({(enh_entropy-orig_entropy)/orig_entropy*100:+.1f}%) |

## ğŸ”´ é«˜é¢‘ Pattern (>1%)

å…± {len(high_freq)} ç§ï¼Œå æ€»é‡ {sum(p[2] for p in high_freq):.1f}%

| æ’å | Pattern | æ•°é‡ | å æ¯” |
|------|---------|------|------|
"""
    
    for i, (pattern, count, pct) in enumerate(high_freq, 1):
        md += f"| {i} | `{pattern}` | {count:,} | {pct:.2f}% |\n"
    
    md += f"""
## ğŸŸ¡ ä¸­é¢‘ Pattern (0.1%-1%)

å…± {len(medium_freq)} ç§ï¼Œå æ€»é‡ {sum(p[2] for p in medium_freq):.1f}%

| æ’å | Pattern | æ•°é‡ | å æ¯” |
|------|---------|------|------|
"""
    
    for i, (pattern, count, pct) in enumerate(medium_freq, 1):
        md += f"| {i} | `{pattern}` | {count:,} | {pct:.2f}% |\n"
    
    md += f"""
## ğŸŸ¢ ä½é¢‘ Pattern (<0.1%)

å…± {len(low_freq)} ç§ï¼Œå æ€»é‡ {sum(p[2] for p in low_freq):.1f}%

<details>
<summary>ç‚¹å‡»å±•å¼€å‰ 50 ç§</summary>

| æ’å | Pattern | æ•°é‡ | å æ¯” |
|------|---------|------|------|
"""
    
    for i, (pattern, count, pct) in enumerate(low_freq[:50], 1):
        md += f"| {i} | `{pattern}` | {count:,} | {pct:.3f}% |\n"
    
    if len(low_freq) > 50:
        md += f"\n*... è¿˜æœ‰ {len(low_freq) - 50} ç§ä½é¢‘ pattern*\n"
    
    md += """
</details>

## ğŸ¯ å…³é”® Pattern å˜åŒ–

"""
    
    # å…³é”® pattern å¯¹æ¯”
    key_patterns = ['thinkâ†’actâ†’speech', 'actâ†’thinkâ†’speech', 'thinkâ†’speech', 'actâ†’speech', 'speech']
    md += "| Pattern | åŸå§‹å æ¯” | å¢å¼ºåå æ¯” | å˜åŒ– |\n"
    md += "|---------|----------|------------|------|\n"
    
    for pattern in key_patterns:
        orig_pct = original_patterns.get(pattern, 0) / orig_total * 100 if orig_total > 0 else 0
        enh_pct = enhanced_patterns.get(pattern, 0) / total_dialogues * 100 if total_dialogues > 0 else 0
        change = enh_pct - orig_pct
        emoji = "âœ…" if change < 0 else ("âš ï¸" if change > 5 else "â–")
        md += f"| `{pattern}` | {orig_pct:.2f}% | {enh_pct:.2f}% | {change:+.2f}% {emoji} |\n"
    
    md += f"""
## ğŸ“Š æ¯æ ·æœ¬ Pattern å¤šæ ·æ€§åˆ†å¸ƒ

| Pattern ç§ç±»æ•° | æ ·æœ¬æ•° | å æ¯” |
|----------------|--------|------|
"""
    
    pattern_count_dist = Counter(patterns_per_sample)
    for n in sorted(pattern_count_dist.keys()):
        count = pattern_count_dist[n]
        pct = count / total_samples * 100
        md += f"| {n} ç§ | {count:,} | {pct:.2f}% |\n"
    
    md += f"""
## ğŸ“‹ æ€»ç»“

1. **Pattern å¤šæ ·æ€§**: å…± {len(enhanced_patterns)} ç§ä¸åŒ pattern
2. **é¦™å†œç†µ**: {enh_entropy:.4f} (åŸå§‹: {orig_entropy:.4f}ï¼Œæå‡ {(enh_entropy-orig_entropy)/orig_entropy*100:.1f}%)
3. **ä¸»è¦ pattern `thinkâ†’actâ†’speech`**: å æ¯” {enhanced_patterns.get('thinkâ†’actâ†’speech', 0)/total_dialogues*100:.2f}%
4. **å¹³å‡æ¯æ ·æœ¬ä½¿ç”¨**: {avg_patterns:.2f} ç§ä¸åŒ pattern
5. **é«˜é¢‘ pattern (>1%)**: {len(high_freq)} ç§ï¼Œå  {sum(p[2] for p in high_freq):.1f}%

---

*æ­¤æŠ¥å‘Šç”± `analyze_pattern_diversity.py` è‡ªåŠ¨ç”Ÿæˆ*
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(md)
    
    return md


def main():
    parser = argparse.ArgumentParser(description='åˆ†æ Pattern å¤šæ ·æ€§')
    parser.add_argument('--input', '-i', type=str, 
                        default='/path/to/data/example',
                        help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', type=str,
                        default='/path/to/data/example',
                        help='è¾“å‡º Markdown æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    print(f"åˆ†ææ–‡ä»¶: {args.input}")
    stats = analyze_data(args.input)
    
    print(f"\nç”ŸæˆæŠ¥å‘Š: {args.output}")
    md = generate_markdown_report(stats, args.output)
    
    print("\nâœ… å®Œæˆï¼")
    print(f"   - æ€»æ ·æœ¬: {stats['total_samples']:,}")
    print(f"   - æ€»å¯¹è¯: {stats['total_dialogues']:,}")
    print(f"   - Pattern ç§ç±»: {len(stats['enhanced_patterns'])}")


if __name__ == '__main__':
    main()

