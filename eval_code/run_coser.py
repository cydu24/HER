#!/usr/bin/env python3
"""
CoSER GCA Evaluation Script (Multi-Agent Version)
CoSER GCA è¯„æµ‹è¿è¡Œè„šæœ¬ (Multi-Agent ç‰ˆæœ¬)

Uses the complete multi-agent implementation from benchmarks/multi_turn/coser:
ä½¿ç”¨ benchmarks/multi_turn/coser çš„å®Œæ•´å¤š Agent å®ç°ï¼š
- Each character has independent dialogue history / æ¯ä¸ªè§’è‰²æœ‰ç‹¬ç«‹çš„å¯¹è¯å†å²
- For self: keep system_thinking + role_thinking / ç»™è‡ªå·±ï¼šä¿ç•™ system_thinking + role_thinking
- For others: remove system_thinking, keep role_thinking / ç»™åˆ«äººï¼šç§»é™¤ system_thinkingï¼Œä¿ç•™ role_thinking
- For others (full clean): remove all inner thoughts / ç»™åˆ«äººï¼ˆå®Œå…¨æ¸…ç†ï¼‰ï¼šç§»é™¤æ‰€æœ‰å†…å¿ƒæƒ³æ³•

Usage / ä½¿ç”¨æ–¹å¼:
    # Full evaluation / å®Œæ•´è¯„æµ‹
    python run_coser.py --actor j-xxx --max-rounds 20 --num-samples 1
    
    # NSP random mode (skip NSP model, randomly select next speaker)
    # NSP éšæœºæ¨¡å¼ï¼ˆä¸è°ƒç”¨ NSP æ¨¡å‹ï¼Œç›´æ¥éšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªè§’è‰²ï¼‰
    python run_coser.py --actor j-xxx --nsp-mode random
    
    # Simulation only / åªè¿è¡Œæ¨ç†
    python run_coser.py --actor j-xxx --simulation-only
    
    # Enable verbose logging / å¯ç”¨è¯¦ç»†æ—¥å¿—
    FULL_LOG=1 python run_coser.py --actor j-xxx --verbose
"""

import os
import sys
import json
import yaml
import argparse
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from models import ModelFactory
from benchmarks.multi_turn.coser.benchmark import (
    CoSERBenchmark,
    CoSERConfig,
    ENVIRONMENT,
    NSP
)

# å®Œæ•´æ—¥å¿—æ¨¡å¼
FULL_LOG = os.environ.get('FULL_LOG', '0') == '1'


def get_default_judge_config(models_config_path: str = "configs/models.yaml") -> Dict:
    """ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤çš„ Judge é…ç½®"""
    try:
        with open(models_config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        judge_config = config.get('judge', {})
        coser_config = judge_config.get('coser', {})
        
        return {
            'default': judge_config.get('default', 'qwen3-235B'),
            'judge': coser_config.get('judge', judge_config.get('default', 'qwen3-235B')),
            'nsp': coser_config.get('nsp', judge_config.get('default', 'qwen3-235B')),
            'env': coser_config.get('env', judge_config.get('default', 'qwen3-235B')),
        }
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å– Judge é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        return {
            'default': 'qwen3-235B',
            'judge': 'qwen3-235B',
            'nsp': 'qwen3-235B',
            'env': 'qwen3-235B',
        }


class CoSERRunner:
    """CoSER Evaluation Runner (Multi-Agent Version) / CoSER è¯„æµ‹è¿è¡Œå™¨ (Multi-Agent ç‰ˆæœ¬)"""
    
    def __init__(
        self,
        actor_model_name: str,
        judge_model_name: str = "qwen3-235B",
        env_model_name: str = None,
        nsp_model_name: str = None,
        nsp_mode: str = "model",
        models_config: str = "configs/models.yaml",
        data_path: str = None,
        roleplay_format: str = "her",
        max_rounds: int = 10,
        max_tokens: int = 4096,
        temperature: float = 0.7,
        output_dir: str = "results/coser",
        verbose: bool = False,
        display_name: str = None,
        eval_remove_role_thinking: bool = False,  # æ¶ˆèå®éªŒï¼šè¯„ä¼°æ—¶æ˜¯å¦ç§»é™¤ role_thinking
        enable_cache: bool = True,  # æ˜¯å¦å¯ç”¨ç¼“å­˜
        skip_cache: bool = False  # æ˜¯å¦è·³è¿‡å·²æœ‰ç¼“å­˜
    ):
        self.actor_model_name = actor_model_name
        self.judge_model_name = judge_model_name
        self.env_model_name = env_model_name or judge_model_name
        self.nsp_model_name = nsp_model_name or judge_model_name
        self.nsp_mode = nsp_mode
        self.models_config = models_config
        self.roleplay_format = roleplay_format
        self.max_rounds = max_rounds
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.output_dir = output_dir
        self.verbose = verbose
        # æ¸…ç† display_name ä¸­çš„ / å­—ç¬¦ï¼ˆé¿å…è·¯å¾„é—®é¢˜ï¼‰
        self.display_name = (display_name or actor_model_name).replace('/', '_')
        self.eval_remove_role_thinking = eval_remove_role_thinking
        self.enable_cache = enable_cache
        self.skip_cache = skip_cache
        
        # Cache ç›®å½•
        self.cache_dir = Path(output_dir) / "cache" / "inference"
        if enable_cache:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            print(f"   ğŸ“ Cacheç›®å½•: {self.cache_dir}")
        
        # ç¡®å®šæ•°æ®è·¯å¾„
        if data_path:
            self.data_path = data_path
        else:
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºè„šæœ¬æ‰€åœ¨ç›®å½•
            script_dir = os.path.dirname(os.path.abspath(__file__))
            base_path = os.path.join(script_dir, "data/coser")
            if roleplay_format in ["her", "her_nosys", "her_with_systhink", "her_without_systhink", "qwen", "api"]:
                self.data_path = os.path.join(base_path, "test_set_her.json")
            else:
                self.data_path = os.path.join(base_path, "test_set.json")
        
        # åŠ è½½æ¨¡å‹
        print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹...")
        self.actor_model = ModelFactory.get_model(models_config, actor_model_name)
        self.judge_model = ModelFactory.get_model(models_config, judge_model_name)
        
        if self.env_model_name != judge_model_name:
            self.env_model = ModelFactory.get_model(models_config, self.env_model_name)
        else:
            self.env_model = self.judge_model
            
        if self.nsp_model_name != judge_model_name:
            self.nsp_model = ModelFactory.get_model(models_config, self.nsp_model_name)
        else:
            self.nsp_model = self.judge_model
        
        print(f"   âœ… Actor: {actor_model_name}")
        print(f"   âœ… Judge: {judge_model_name}")
        print(f"   âœ… Env: {self.env_model_name}")
        print(f"   âœ… NSP: {self.nsp_model_name} (mode: {nsp_mode})")
        
        # åˆ›å»º CoSER é…ç½®
        self.config = CoSERConfig(
            name="coser",
            data_path=self.data_path,
            output_dir=output_dir,
            max_tokens=max_tokens,
            temperature=temperature,
            actor_model=actor_model_name,
            env_model=self.env_model_name,
            nsp_model=self.nsp_model_name,
            judge_model=judge_model_name,
            max_rounds=max_rounds,
            nsp_mode=nsp_mode,
            model_type=roleplay_format,
            eval_remove_role_thinking=eval_remove_role_thinking
        )
        
        # åˆ›å»º Benchmark
        self.benchmark = CoSERBenchmark(self.config)
        print(f"   âœ… æ•°æ®: {self.data_path} ({len(self.benchmark.data)} æ¡)")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
    
    def _get_cache_path(self, sample_id: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # æ¸…ç† sample_id ä¸­çš„éæ³•å­—ç¬¦
        safe_id = sample_id.replace('/', '_').replace('\\', '_').replace(':', '_')
        return self.cache_dir / f"{safe_id}.json"
    
    def _load_cache(self, sample_id: str) -> Optional[Dict]:
        """ä»ç¼“å­˜åŠ è½½ç»“æœ"""
        if not self.enable_cache or self.skip_cache:
            return None
        cache_path = self._get_cache_path(sample_id)
        if cache_path.exists():
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"   âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ {sample_id}: {e}")
        return None
    
    def _save_cache(self, sample_id: str, result: Dict):
        """ä¿å­˜ç»“æœåˆ°ç¼“å­˜"""
        if not self.enable_cache:
            return
        cache_path = self._get_cache_path(sample_id)
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False)
        except Exception as e:
            print(f"   âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥ {sample_id}: {e}")
    
    async def run_simulation(self, num_samples: int = None, sample_offset: int = 0, workers: int = 1) -> List[Dict]:
        """è¿è¡Œæ¨ç†é˜¶æ®µï¼ˆæ”¯æŒå¹¶å‘å’Œç¼“å­˜ï¼‰"""
        print(f"\nğŸ­ å¼€å§‹ Simulation (æ¨ç†)...")
        print(f"   æœ€å¤§è½®æ•°: {self.max_rounds}")
        print(f"   NSP æ¨¡å¼: {self.nsp_mode}")
        print(f"   å¹¶å‘æ•°: {workers}")
        print(f"   ç¼“å­˜: {'å¯ç”¨' if self.enable_cache else 'ç¦ç”¨'}")
        if sample_offset > 0:
            print(f"   æ ·æœ¬åç§»: {sample_offset}")
        
        # æ”¯æŒ sample_offset åˆ†æ‰¹
        all_data = self.benchmark.data
        if sample_offset > 0:
            all_data = all_data[sample_offset:]
        data = all_data[:num_samples] if num_samples else all_data
        
        print(f"   å®é™…æ ·æœ¬èŒƒå›´: {sample_offset} - {sample_offset + len(data) - 1} (å…± {len(data)} æ¡)")
        
        # æ£€æŸ¥ç¼“å­˜ï¼Œè·³è¿‡å·²å®Œæˆçš„æ ·æœ¬
        cached_count = 0
        pending_indices = []
        results = [None] * len(data)
        
        for idx, sample in enumerate(data):
            sample_id = self.benchmark.get_sample_id(sample, idx + sample_offset)
            cached = self._load_cache(sample_id)
            if cached and cached.get('status') == 'completed':
                results[idx] = cached
                cached_count += 1
            else:
                pending_indices.append(idx)
        
        if cached_count > 0:
            print(f"   ğŸ“¦ ä»ç¼“å­˜æ¢å¤: {cached_count} æ¡")
        print(f"   â³ å¾…å¤„ç†: {len(pending_indices)} æ¡")
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(workers)
        completed = [cached_count]  # ä½¿ç”¨åˆ—è¡¨æ¥åœ¨é—­åŒ…ä¸­ä¿®æ”¹
        
        async def process_sample(idx: int, sample: Dict):
            async with semaphore:
                sample_id = self.benchmark.get_sample_id(sample, idx + sample_offset)
                
                try:
                    result = await self.benchmark.run_inference_single(
                        sample=sample,
                        model=self.actor_model,
                        env_model=self.env_model,
                        nsp_model=self.nsp_model
                    )
                    result_dict = {
                        "sample_id": sample_id,
                        "sample": sample,
                        "dialogue": result.dialogue,
                        "metadata": result.metadata,
                        "status": "completed"
                    }
                    results[idx] = result_dict
                    
                    # ä¿å­˜ç¼“å­˜
                    self._save_cache(sample_id, result_dict)
                    
                    completed[0] += 1
                    print(f"   âœ… [{completed[0]}/{len(data)}] {sample_id}: {len(result.dialogue)} è½®å¯¹è¯")
                except Exception as e:
                    print(f"   âŒ [{idx+1}/{len(data)}] {sample_id} å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    results[idx] = {
                        "sample_id": sample_id,
                        "sample": sample,
                        "error": str(e),
                        "status": "failed"
                    }
                    completed[0] += 1
        
        # åªå¤„ç†æœªç¼“å­˜çš„æ ·æœ¬
        tasks = [process_sample(idx, data[idx]) for idx in pending_indices]
        await asyncio.gather(*tasks)
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        sim_path = f"{self.output_dir}/simulation_{self.display_name}_{timestamp}.json"
        
        output = {
            "config": {
                "actor_model": self.actor_model_name,
                "roleplay_format": self.roleplay_format,
                "max_rounds": self.max_rounds,
                "nsp_mode": self.nsp_mode,
                "timestamp": timestamp
            },
            "results": results
        }
        
        with open(sim_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Simulation å®Œæˆ! ä¿å­˜åˆ°: {sim_path}")
        return results
    
    async def run_evaluation(self, simulation_results: List[Dict] = None, simulation_path: str = None) -> Dict:
        """è¿è¡Œè¯„ä¼°é˜¶æ®µ"""
        print(f"\nğŸ“Š å¼€å§‹ Evaluation (è¯„ä¼°)...")
        
        # åŠ è½½æ¨ç†ç»“æœ
        if simulation_results is None:
            if simulation_path is None:
                raise ValueError("å¿…é¡»æä¾› simulation_results æˆ– simulation_path")
            with open(simulation_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                simulation_results = data.get('results', data)
        
        # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
        evaluations = []
        all_scores = []
        
        for idx, result in enumerate(simulation_results):
            if result.get('status') == 'failed':
                continue
            
            sample_id = result.get('sample_id', f'sample_{idx}')
            print(f"   è¯„ä¼°: {sample_id}")
            
            try:
                # è°ƒç”¨ benchmark çš„è¯„ä¼°æ–¹æ³•
                from benchmarks.multi_turn.coser.benchmark import InferenceResult
                
                # æ„å»º InferenceResult å¯¹è±¡
                inference_result = InferenceResult(
                    sample_id=sample_id,
                    model_name=self.actor_model_name,
                    input_data=result.get('sample', {}),
                    dialogue=result.get('dialogue', []),
                    metadata=result.get('metadata', {}),
                    status='completed'
                )
                
                # è°ƒç”¨è¯„ä¼°
                eval_result = await self.benchmark.evaluate_single(
                    inference_result=inference_result,
                    judge_model=self.judge_model
                )
                
                evaluations.append({
                    "sample_id": sample_id,
                    "scores": eval_result.scores,
                    "details": eval_result.details
                })
                all_scores.append(eval_result.scores)
                
                print(f"   âœ… {sample_id}: avg={eval_result.scores.get('avg', 0):.1f}")
                
            except Exception as e:
                import traceback
                print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")
                traceback.print_exc()
                evaluations.append({
                    "sample_id": sample_id,
                    "scores": {},
                    "error": str(e)
                })
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        final_scores = {}
        if all_scores:
            for key in all_scores[0].keys():
                scores = [s.get(key, 0) for s in all_scores if key in s]
                if scores:
                    final_scores[key] = sum(scores) / len(scores)
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        eval_path = f"{self.output_dir}/evaluation_{self.display_name}_{timestamp}.json"
        
        output = {
            "config": {
                "actor_model": self.actor_model_name,
                "judge_model": self.judge_model_name,
                "timestamp": timestamp
            },
            "scores": final_scores,
            "evaluations": evaluations
        }
        
        with open(eval_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ±‡æ€»åˆ†æ•°
        if final_scores:
            print(f"\nğŸ“Š è¯„ä¼°æ±‡æ€»:")
            for dim, score in final_scores.items():
                print(f"   {dim}: {score:.2f}")
        
        print(f"\nâœ… Evaluation å®Œæˆ! ä¿å­˜åˆ°: {eval_path}")
        return final_scores
    
    async def run(
        self,
        num_samples: int = None,
        sample_offset: int = 0,
        workers: int = 1,
        simulation_only: bool = False,
        evaluation_only: bool = False,
        simulation_file: str = None
    ):
        """è¿è¡Œå®Œæ•´è¯„æµ‹æµç¨‹"""
        if evaluation_only:
            if simulation_file:
                return await self.run_evaluation(simulation_path=simulation_file)
            else:
                raise ValueError("evaluation-only æ¨¡å¼éœ€è¦æŒ‡å®š --simulation-file")
        
        # è¿è¡Œæ¨ç†
        results = await self.run_simulation(num_samples, sample_offset=sample_offset, workers=workers)
        
        if simulation_only:
            return results
        
        # è¿è¡Œè¯„ä¼°
        return await self.run_evaluation(results)


def main():
    parser = argparse.ArgumentParser(description='CoSER GCA è¯„æµ‹ (Multi-Agent ç‰ˆæœ¬)')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--actor', type=str, required=True, help='å¾…è¯„æµ‹çš„è§’è‰²æ‰®æ¼”æ¨¡å‹')
    parser.add_argument('--display-name', type=str, default=None, help='æ¨¡å‹æ˜¾ç¤ºåç§°')
    parser.add_argument('--judge', type=str, default=None, help='è¯„ä¼°ç”¨çš„ Judge æ¨¡å‹')
    parser.add_argument('--env', type=str, default=None, help='ç¯å¢ƒæè¿°æ¨¡å‹')
    parser.add_argument('--nsp', type=str, default=None, help='ä¸‹ä¸€è¯´è¯äººé¢„æµ‹æ¨¡å‹')
    parser.add_argument('--nsp-mode', type=str, default='model', choices=['model', 'random'],
                        help='NSPæ¨¡å¼: model=ä½¿ç”¨æ¨¡å‹é¢„æµ‹, random=éšæœºé€‰æ‹©è§’è‰²')
    
    # æ•°æ®é…ç½®
    parser.add_argument('--data', type=str, default=None, help='æµ‹è¯•æ•°æ®è·¯å¾„')
    parser.add_argument('--format', type=str, default='her',
                        choices=['her', 'her_nosys', 'her_with_systhink', 'her_without_systhink', 
                                 'qwen', 'coser', 'api', 'llama3'],
                        help='è§’è‰²æ‰®æ¼”æ ¼å¼')
    parser.add_argument('--max-rounds', type=int, default=10, help='æœ€å¤§å¯¹è¯è½®æ•°')
    parser.add_argument('--max-tokens', type=int, default=4096, help='å•æ¬¡ç”Ÿæˆæœ€å¤§ tokens')
    parser.add_argument('--num-samples', type=int, default=None, help='æµ‹è¯•æ ·æœ¬æ•°')
    parser.add_argument('--sample-offset', type=int, default=0, help='è·³è¿‡å‰Næ¡æ ·æœ¬ (ç”¨äºåˆ†æ‰¹)')
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument('--simulation-only', action='store_true', help='åªè¿è¡Œæ¨ç†')
    parser.add_argument('--evaluation-only', action='store_true', help='åªè¿è¡Œè¯„ä¼°')
    parser.add_argument('--simulation-file', type=str, default=None, help='æŒ‡å®š simulation æ–‡ä»¶è·¯å¾„')
    
    # æ¶ˆèå®éªŒ
    parser.add_argument('--eval-remove-role-thinking', action='store_true', 
                        help='æ¶ˆèå®éªŒï¼šè¯„ä¼°æ—¶ç§»é™¤ role_thinking (é»˜è®¤ä¿ç•™)')
    
    # è¾“å‡ºé…ç½®
    parser.add_argument('--output', type=str, default='results/coser', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--log-dir', type=str, default=None, help='æ—¥å¿—ç›®å½• (é»˜è®¤åŒ output)')
    parser.add_argument('--models-config', type=str, default='configs/models.yaml', help='æ¨¡å‹é…ç½®æ–‡ä»¶')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¾“å‡ºè¯¦ç»†æ—¥å¿—')
    parser.add_argument('--full-log', action='store_true', help='è¾“å‡ºå®Œæ•´æ—¥å¿—')
    parser.add_argument('--workers', type=int, default=100, help='æ¨ç†å¹¶å‘æ•° (é»˜è®¤100)')
    
    # Cache é…ç½®
    parser.add_argument('--enable-cache', action='store_true', default=True, help='å¯ç”¨ç¼“å­˜ (é»˜è®¤å¯ç”¨)')
    parser.add_argument('--no-cache', action='store_true', help='ç¦ç”¨ç¼“å­˜')
    parser.add_argument('--skip-cache', action='store_true', help='è·³è¿‡å·²æœ‰ç¼“å­˜é‡æ–°è¿è¡Œ')
    
    args = parser.parse_args()
    
    # è·å–é»˜è®¤ Judge é…ç½®
    default_judge = get_default_judge_config(args.models_config)
    
    judge_model = args.judge or default_judge['judge']
    env_model = args.env or default_judge['env']
    nsp_model = args.nsp or default_judge['nsp']
    
    print(f"\nğŸ”§ é…ç½®:")
    print(f"   Actor: {args.actor}")
    print(f"   Judge: {judge_model}")
    print(f"   NSP:   {nsp_model} (mode: {args.nsp_mode})")
    print(f"   Env:   {env_model}")
    print(f"   Format: {args.format}")
    print(f"   Max Rounds: {args.max_rounds}")
    
    # è®¾ç½® FULL_LOG ç¯å¢ƒå˜é‡
    if args.verbose or args.full_log:
        os.environ['FULL_LOG'] = '1'
    
    # Cache é…ç½®
    enable_cache = args.enable_cache and not args.no_cache
    
    runner = CoSERRunner(
        actor_model_name=args.actor,
        judge_model_name=judge_model,
        env_model_name=env_model,
        nsp_model_name=nsp_model,
        nsp_mode=args.nsp_mode,
        models_config=args.models_config,
        data_path=args.data,
        roleplay_format=args.format,
        max_rounds=args.max_rounds,
        max_tokens=args.max_tokens,
        output_dir=args.output,
        verbose=args.verbose,
        display_name=args.display_name,
        eval_remove_role_thinking=args.eval_remove_role_thinking,
        enable_cache=enable_cache,
        skip_cache=args.skip_cache
    )
    
    # è¿è¡Œ
    asyncio.run(runner.run(
        num_samples=args.num_samples,
        sample_offset=args.sample_offset,
        workers=args.workers,
        simulation_only=args.simulation_only,
        evaluation_only=args.evaluation_only,
        simulation_file=args.simulation_file
    ))


if __name__ == "__main__":
    main()

