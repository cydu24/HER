"""
Reward Model Evaluation Script / å¥–åŠ±æ¨¡å‹è¯„ä¼°è„šæœ¬

Process model inference results and extract evaluation scores.
å¤„ç†æ¨¡å‹æ¨ç†ç»“æœå¹¶æå–è¯„ä¼°åˆ†æ•°ã€‚
"""

import os
import json
import glob
from tqdm import tqdm
from typing import List, Dict, Any
import numpy as np
import re
import random

def process_file(file_path: str, model_name: str) -> List[Dict[str, Any]]:
    """Process single jsonl file / å¤„ç†å•ä¸ªjsonlæ–‡ä»¶"""
    import json  # ç§»åˆ°å‡½æ•°å¼€å¤´ï¼Œé¿å…ä½œç”¨åŸŸé—®é¢˜
    from tqdm import tqdm
    import os
    
    processed_data = []
    token_info = []
    failed_items = []
    skipped_items = []  # æ–°å¢ï¼šä¿å­˜è·³è¿‡çš„æ•°æ®
    
    # è·å–æ–‡ä»¶åç”¨äºè¿›åº¦æ¡æè¿°
    file_name = os.path.basename(file_path)
    
    # é¦–å…ˆè®¡ç®—æ€»è¡Œæ•°
    with open(file_path, 'r', encoding='utf-8') as f:
        total_lines = sum(1 for line in f if line.strip())
    
    with open(file_path, 'r', encoding='utf-8') as f:
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        pbar = tqdm(enumerate(f, 1), total=total_lines, desc=f"å¤„ç† {file_name}", leave=False)
        for line_num, line in pbar:
                # æ›´æ–°è¿›åº¦æ¡ç»Ÿè®¡ä¿¡æ¯
                pbar.set_postfix({
                    'æˆåŠŸ': len(processed_data), 
                    'è·³è¿‡': len(skipped_items),
                    'å¤±è´¥': len(failed_items)
                })
          
                line = line.strip()
                if not line:  # è·³è¿‡ç©ºè¡Œ
                    continue
                    
                data = json.loads(line)
                # æå–responseå’Œthink
                think = ""
                response = ""

                if model_name == 'claude_3_7_sonnet_20250219' or model_name == 'claude_opus_4_20250514' or model_name == 'claude_sonnet_4_20250514':
                    model_req = json.loads(data['vulcan_output']['model_req'])
                    for content in model_req['content']:
                        if content['type'] == 'thinking':
                            data['think'] = content['thinking'].strip()
                        if content['type'] == 'text':
                            data['response'] = content['text'].strip()
                elif model_name == "model":
                    try:
                        # æ£€æŸ¥statusæ˜¯å¦ä¸ºcompletedï¼ˆæ”¯æŒå¤šç§statusä½ç½®ï¼‰
                        status = None
                        if 'model_request_output' in data['vulcan_output']:
                            status = data['vulcan_output']['model_request_output'].get('status', '')
                        elif 'status' in data['vulcan_output']:
                            status = data['vulcan_output'].get('status', '')
                        
                        # è·³è¿‡écompletedçŠ¶æ€ï¼ˆåŒ…æ‹¬incompleteã€failedç­‰ï¼‰
                        if status and status.lower() not in ['completed', 'complete']:
                            skipped_items.append({
                                'line_num': line_num,
                                'reason': f'statusä¸æ˜¯completed: {status}',
                                'data': data
                            })
                            continue
                        
                        # æ£€æŸ¥outputçš„é•¿åº¦
                        output_list = data['vulcan_output'].get('output', [])
                        if not output_list or len(output_list) < 2:
                            skipped_items.append({
                                'line_num': line_num,
                                'reason': f'outputåªæœ‰{len(output_list)}ä¸ªå…ƒç´ ï¼Œéœ€è¦è‡³å°‘2ä¸ª',
                                'data': data
                            })
                            continue
                            
                        # è·å–reasoningæ•°æ®
                        reasoning_data = output_list[0].get("summary", [])
                        
                        # æ£€æŸ¥summaryæ˜¯å¦ä¸ºç©º
                        if not reasoning_data:
                            # é™é»˜è·³è¿‡ï¼Œåªè®°å½•åˆ°skipped_items
                            # print(f"\nâš ï¸ summaryä¸ºç©ºï¼Œè·³è¿‡...")
                            skipped_items.append({
                                'line_num': line_num,
                                'reason': 'summaryä¸ºç©º',
                                'data': data
                            })
                            continue
                        
                        # å®šä¹‰æœ‰åºçš„è¿æ¥è¯ç»“æ„
                        ordinal_connectors = {
                            0: [
                                 "First of all,", "To begin with,", "Initially,", 
                                "Let me start by noting that", "To start,", "At the outset,"
                            ],
                            1: [
                                 "Next,", "Then,", "Following this,", 
                                "Moving on to the second point,", "Secondly,", "After that,"
                            ],
                            2: [
                                 "Furthermore,", "Additionally,", "Thirdly,",
                                "Considering another aspect,", "Also worth noting,", "In addition to this,"
                            ],
                            3: [
                                 "Moreover,", "Building on this,", "Fourthly,",
                                "What's more,", "Equally important,", "Along these lines,"
                            ],
                            4: [
                                "Subsequently,", "In addition,", "Fifthly,",
                                "Further to this point,", "Another key consideration,", "Beyond that,"
                            ],
                            "middle": [
                                "Continuing with the analysis,", "Moving forward,", "Also,", 
                                "Let me also consider that", "It's also worth examining how", "Delving deeper,",
                                "On a related note,", "In a similar manner,", "In the same vein,", 
                                "Expanding on this,", "To elaborate further,", "Looking more closely,"
                            ],
                            "penultimate": [
                                "Before concluding,", "As a final consideration,", "Lastly,", 
                                "To wrap up the analysis,", "One final point to consider,",
                                "In the final analysis,", "As a closing thought,", "To summarize the key findings,"
                            ],
                        }
                        
                        # å¼€å§‹æ„å»ºæ€è€ƒæ–‡æœ¬
                        think_parts = []
                        
                        # å¤šæ ·åŒ–çš„å¼€åœºç™½
                        openings = [
                            "Alright, let me analyze this task now.",
                            "Let me carefully examine this evaluation task.",
                            "I need to thoroughly analyze these candidates.",
                            "Let me break down this comparison systematically.",
                            "Time to evaluate these responses comprehensively.",
                            "Let me approach this analysis methodically.",
                            "I'll now conduct a detailed assessment of these candidates.",
                            "Let me work through this evaluation step by step."
                        ]
                        think_parts.append(random.choice(openings) + "\n\n")
                        
                        # å¤„ç†æ¯ä¸ªsummaryé¡¹
                        total_items = len(reasoning_data)
                        for i, item in enumerate(reasoning_data):
                            # æå–æ–‡æœ¬å†…å®¹ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰
                            text_content = "\n\n".join(item['text'].split("\n\n")[1:])
                            
                            # é€‰æ‹©åˆé€‚çš„è¿æ¥è¯
                            if i == 0:
                                # ç¬¬ä¸€é¡¹
                                connector = random.choice(ordinal_connectors[0])
                                think_parts.append(f"{connector} {text_content}")
                            elif i == total_items - 1 and total_items > 2:
                                # æœ€åä¸€é¡¹ï¼ˆå¦‚æœè¶…è¿‡2é¡¹ï¼‰
                                connector = random.choice(ordinal_connectors["penultimate"])
                                think_parts.append(f"\n\n{connector} {text_content}")
                            elif i <= 4 and i in ordinal_connectors:
                                # ç¬¬2-5é¡¹ä½¿ç”¨å¯¹åº”çš„åºæ•°è¿æ¥è¯
                                connector = random.choice(ordinal_connectors[i])
                                think_parts.append(f"\n\n{connector} {text_content}")
                            else:
                                # ä¸­é—´é¡¹ä½¿ç”¨é€šç”¨è¿æ¥è¯
                                connector = random.choice(ordinal_connectors["middle"])
                                think_parts.append(f"\n\n{connector} {text_content}")
                        
                        # æ·»åŠ ç»“æŸè¯­
                        final_phrases = [
                            "\n\nFinally, after analyzing all aspects, I've reached my conclusion.",
                            "\n\nFinally, having considered all dimensions, my evaluation is complete.",
                            "\n\nFinally, based on this comprehensive analysis, I can now determine the outcome.",
                            "\n\nFinally, with all factors weighed, I've arrived at my decision.",
                            "\n\nIn conclusion, after careful consideration of all the evidence, I have my answer.",
                            "\n\nUltimately, taking everything into account, the better response is clear.",
                            "\n\nTo conclude, having examined all the relevant principles, I can make my judgment.",
                            "\n\nAll things considered, I've arrived at a definitive evaluation.",
                            "\n\nHaving weighed all the factors, my assessment is complete.",
                            "\n\nBased on this thorough analysis, I can now make an informed decision."
                        ]
                        think_parts.append(random.choice(final_phrases))
                        
                        # å¤šæ ·åŒ–çš„è¾“å‡ºè½¬æ¢è¯­å¥
                        output_transitions = [
                            "\n\nOkay, now let me generate the output.",
                            "\n\nNow I'll structure this evaluation in the required JSON format.",
                            "\n\nLet me now format this analysis into the proper output structure.",
                            "\n\nTime to compile this assessment into the final response.",
                            "\n\nI'll now generate the structured evaluation output.",
                            "\n\nLet me produce the formal evaluation result.",
                            "\n\nNow to create the detailed JSON output for this evaluation.",
                            "\n\nWith the analysis complete, I'll generate the formatted response."
                        ]
                        think_parts.append(random.choice(output_transitions))
                        
                        # æ‹¼æ¥æ‰€æœ‰éƒ¨åˆ†
                        data['think'] = "".join(think_parts)
                        
                        # å°è¯•è·å–responseï¼ˆå¯èƒ½åœ¨output[1]æˆ–output[2]ä¸­ï¼‰
                        response_found = False
                        
                        # å…ˆå°è¯•output[2]ï¼ˆ3ä¸ªå…ƒç´ çš„æƒ…å†µï¼‰
                        if len(output_list) > 2 and 'content' in output_list[2]:
                            try:
                                data['response'] = output_list[2]['content'][0]['text']
                                response_found = True
                            except (KeyError, IndexError):
                                pass
                        
                        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•output[1]ï¼ˆ2ä¸ªå…ƒç´ çš„æƒ…å†µï¼‰
                        if not response_found and len(output_list) > 1 and 'content' in output_list[1]:
                            try:
                                data['response'] = output_list[1]['content'][0]['text']
                                response_found = True
                            except (KeyError, IndexError):
                                pass
                        
                        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè·³è¿‡
                        if not response_found:
                            skipped_items.append({
                                'line_num': line_num,
                                'reason': 'åœ¨outputä¸­æ²¡æœ‰æ‰¾åˆ°contentæˆ–text',
                                'data': data
                            })
                            continue
                        
                    except (IndexError, KeyError) as e:
                        print("\n" + "="*80)
                        print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
                        print(f"âŒ é”™è¯¯ä¿¡æ¯: {str(e)}")
                        print("="*80)
                        print("\nğŸ“Š è¾“å‡ºç»“æ„åˆ†æ:")
                        print(f"outputåˆ—è¡¨é•¿åº¦: {len(data['vulcan_output']['output'])}")
                        
                        # æ‰“å°æ¯ä¸ªoutputå…ƒç´ çš„ç»“æ„
                        for idx, output_item in enumerate(data['vulcan_output']['output']):
                            print(f"\n[Output {idx}] çš„é”®:")
                            print(f"  - Keys: {list(output_item.keys())}")
                            
                            # å¦‚æœæœ‰contenté”®ï¼Œæ‰“å°å…¶ç»“æ„
                            if 'content' in output_item:
                                if isinstance(output_item['content'], list):
                                    print(f"  - contentæ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(output_item['content'])}")
                                    if len(output_item['content']) > 0:
                                        print(f"  - content[0]çš„ç±»å‹: {type(output_item['content'][0])}")
                                        if isinstance(output_item['content'][0], dict):
                                            print(f"  - content[0]çš„é”®: {list(output_item['content'][0].keys())}")
                                else:
                                    print(f"  - contentç±»å‹: {type(output_item['content'])}")
                        
                        # æ‰“å°å®Œæ•´çš„outputç»“æ„ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
                        print("\nğŸ“ å®Œæ•´outputç»“æ„ï¼ˆå‰500å­—ç¬¦ï¼‰:")
                        output_str = json.dumps(data['vulcan_output']['output'], indent=2, ensure_ascii=False)
                        print(output_str[:500] + "..." if len(output_str) > 500 else output_str)
                        
                        print("\n" + "="*80)
                        
                        # è·³è¿‡è¿™æ¡æ•°æ®
                        print("âš ï¸ è·³è¿‡è¿™æ¡æ•°æ®ï¼Œç»§ç»­å¤„ç†...")
                        skipped_items.append({
                            'line_num': line_num,
                            'reason': f'å¤„ç†é”™è¯¯: {type(e).__name__}: {str(e)}',
                            'data': data
                        })
                        continue
                processed_data.append(data)
                # å®‰å…¨åˆ é™¤vulcan_outputå­—æ®µ
                if 'vulcan_output' in data:
                    del data['vulcan_output']
               
                
                
    
    return processed_data, token_info, failed_items, skipped_items
'''

Processing gemini_2_5_pro_preview_05_06
model_req: dict_keys(['candidates', 'usageMetadata', 'modelVersion', 'createTime', 'responseId'])
Processing gemini_2_5_pro_preview_05_06: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1467.05it/s]
Processing gemini_2_5_pro_preview_06_05
model_req: dict_keys(['candidates', 'usageMetadata', 'modelVersion', 'createTime', 'responseId'])
{'candidates': [{'content': {'role': 'model', 'parts': [{'text' {'text': 
'''  

# Rayå¤„ç†å‡½æ•°å·²ç§»é™¤ï¼Œç°åœ¨ä½¿ç”¨ä¸²è¡Œå¤„ç†


def extract_field(text: str, start_pattern: str, end_pattern: str) -> str:
    """æå–å­—æ®µå†…å®¹ï¼Œä½¿ç”¨ä¸‹ä¸€ä¸ªå­—æ®µä½œä¸ºç»“æŸæ ‡è®°"""
    try:
        if not text or not isinstance(text, str):
            return ""
            
        start = re.search(start_pattern, text, re.DOTALL)
        if not start:
            return ""
        start_pos = start.end()
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿start_posä¸è¶…å‡ºæ–‡æœ¬é•¿åº¦
        if start_pos >= len(text):
            return ""
        
        end = re.search(end_pattern, text[start_pos:], re.DOTALL)
        if not end:
            return text[start_pos:].strip()
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿end.start()ä¸è¶…å‡ºå‰©ä½™æ–‡æœ¬é•¿åº¦
        if end.start() > len(text[start_pos:]):
            return text[start_pos:].strip()
            
        content = text[start_pos:start_pos + end.start()].strip()
        return content
    except Exception as e:
        print(f"Error extracting field: {e}")
        return ""

def parse_characters(characters_text: str) -> list:
    """è§£æè§’è‰²æ•°ç»„ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    try:
        if not characters_text.strip():
            return []
        
        # æ–¹æ³•1: å°è¯•ç›´æ¥è§£æJSON
        try:
            # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
            cleaned_text = characters_text.strip()
            if not cleaned_text.startswith('['):
                cleaned_text = '[' + cleaned_text
            if not cleaned_text.endswith(']'):
                cleaned_text = cleaned_text + ']'
            
            characters = json.loads(cleaned_text)
            if isinstance(characters, list):
                return characters
        except json.JSONDecodeError:
            pass
        
        # æ–¹æ³•2: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼é€ä¸ªåŒ¹é…è§’è‰²
        character_patterns = [
            # æ ‡å‡†æ ¼å¼ - æ”¯æŒå¤šè¡Œå†…å®¹
            r'\{\s*"name"\s*:\s*"([^"]*)",\s*"motivation"\s*:\s*"(.*?)",\s*"background"\s*:\s*"(.*?)"\s*\}',
            # å®¹é”™æ ¼å¼ - æ”¯æŒå•å¼•å·
            r"\{\s*'name'\s*:\s*'([^']*)',\s*'motivation'\s*:\s*'(.*?)',\s*'background'\s*:\s*'(.*?)'\s*\}",
            # ç®€åŒ–æ ¼å¼
            r'\{\s*"name"\s*:\s*"([^"]*)",\s*"motivation"\s*:\s*"([^"]*)",\s*"background"\s*:\s*"([^"]*)"\s*\}',
        ]
        
        characters_list = []
        for pattern in character_patterns:
            matches = re.findall(pattern, characters_text, re.DOTALL)
            if matches:
                for match in matches:
                    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿matchæœ‰è¶³å¤Ÿçš„å…ƒç´ 
                    if len(match) >= 3:
                        character = {
                            'name': match[0].strip(),
                            'motivation': match[1].strip().replace('\\"', '"'),
                            'background': match[2].strip().replace('\\"', '"')
                        }
                        characters_list.append(character)
                    else:
                        # å¦‚æœå…ƒç´ ä¸è¶³ï¼Œè®°å½•é”™è¯¯å¹¶è·³è¿‡
                        print(f"Warning: Incomplete match found in pattern {pattern}: {match}")
                        continue
                break
        
        return characters_list
        
    except Exception as e:
        print(f"Error parsing characters: {e}")
        return []

def extract_json_response(response_text: str) -> List[Dict[str, Any]]:
    """ä»responseä¸­æå–JSONæ ¼å¼çš„æ•°æ®"""
    try:
        if not response_text or not isinstance(response_text, str):
            return []
        
        # æŸ¥æ‰¾JSONä»£ç å—
        json_pattern = r'```json\s*(\[.*?\])\s*```'
        matches = re.findall(json_pattern, response_text, re.DOTALL)
        
        extracted_items = []
        for match in matches:
            try:
                json_data = json.loads(match)
                if isinstance(json_data, list):
                    extracted_items.extend(json_data)
                elif isinstance(json_data, dict):
                    extracted_items.append(json_data)
            except json.JSONDecodeError as e:
                print(f"JSONè§£æå¤±è´¥: {e}")
                continue
        
        return extracted_items
        
    except Exception as e:
        print(f"æå–JSONå“åº”æ—¶å‡ºé”™: {e}")
        return []

def process_directory(input_dir: str, model_name: str):
    """å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰jsonlæ–‡ä»¶å¹¶å°†ç»“æœå†™å…¥è¾“å‡ºæ–‡ä»¶"""
    
    # è·å–æ‰€æœ‰jsonlæ–‡ä»¶
    jsonl_files = glob.glob(os.path.join(input_dir, "*.jsonl"))
    jsonl_files = [file for file in jsonl_files if not file.endswith('failed.jsonl')]
    print(f"æ‰¾åˆ° {len(jsonl_files)} ä¸ªjsonlæ–‡ä»¶ï¼ˆè·³è¿‡failedæ–‡ä»¶ï¼‰")
    
    all_processed_data = []
    all_token_info = []
    all_failed_items = []
    all_skipped_items = []  # æ–°å¢ï¼šæ”¶é›†æ‰€æœ‰è·³è¿‡çš„æ•°æ®
    
    # ä¸²è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼Œä½¿ç”¨ä¸»è¿›åº¦æ¡
    for file_path in tqdm(jsonl_files, desc="å¤„ç†æ–‡ä»¶", unit="file"):
        result, token_info, failed_items, skipped_items = process_file(file_path, model_name)
        all_processed_data.extend(result)
        all_token_info.extend(token_info)
        all_failed_items.extend(failed_items)
        all_skipped_items.extend(skipped_items)  # æ”¶é›†è·³è¿‡çš„æ•°æ®
    output_path = f'{input_dir}_processed/{model_name}_processed.jsonl'
    print(f'all_processed_data: {len(all_processed_data)}, now write to {output_path}')
    
    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in all_processed_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    # ä¿å­˜å¤„ç†å¤±è´¥çš„æ•°æ®
    if all_failed_items:
        failed_output_path = output_path.replace('.jsonl', '_parse_failed.jsonl')
        with open(failed_output_path, 'w', encoding='utf-8') as f:
            for item in all_failed_items:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"ä¿å­˜äº† {len(all_failed_items)} æ¡è§£æå¤±è´¥çš„æ•°æ®åˆ° {failed_output_path}")
    
    # ä¿å­˜è·³è¿‡çš„æ•°æ®
    if all_skipped_items:
        skipped_output_path = output_path.replace('.jsonl', '_skipped.jsonl')
        with open(skipped_output_path, 'w', encoding='utf-8') as f:
            for item in all_skipped_items:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"ä¿å­˜äº† {len(all_skipped_items)} æ¡è·³è¿‡çš„æ•°æ®åˆ° {skipped_output_path}")
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡
    print("\n" + "="*60)
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡ï¼š")
    print(f"  âœ… æˆåŠŸå¤„ç†: {len(all_processed_data)} æ¡")
    print(f"  âš ï¸ è·³è¿‡æ•°æ®: {len(all_skipped_items)} æ¡")
    print(f"  âŒ å¤±è´¥æ•°æ®: {len(all_failed_items)} æ¡")
    print(f"  ğŸ“ æ€»è®¡: {len(all_processed_data) + len(all_skipped_items) + len(all_failed_items)} æ¡")
    print("="*60 + "\n")
    
    print(f"å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(all_processed_data)} æ¡æ•°æ®ï¼Œå·²å†™å…¥ {output_path}")

    # æå–judge_result
    print(f"all_processed_data: {len(all_processed_data)}, now extract judge_result")

    # æ”¹è¿›çš„æŠ½å–æ¨¡å¼é…ç½®
    def get_field_patterns(model_name: str):
        """æ ¹æ®æ¨¡å‹åç§°è¿”å›å¯¹åº”çš„å­—æ®µæŠ½å–æ¨¡å¼"""
        # æ—§æ ¼å¼æ¨¡å¼
        old_patterns = [
            ('scenario', '"scenario"\\s*:\\s*"', '",\\s*"topic":', 'string'),
            ('topic', '"topic"\\s*:\\s*"', '",\\s*"key_characters":', 'string'),
            ('key_characters', '"key_characters"\\s*:\\s*\\[', '\\]\\s*}\\s*\\]\\s*}', 'character_array')
        ]
        
        # æ–°JSONæ ¼å¼æ¨¡å¼ - æ”¯æŒæå–å¤šä¸ªå­—æ®µ
        json_patterns = [
            ('id', '"id"\\s*:\\s*(\\d+)', ',', 'integer'),
            ('type', '"type"\\s*:\\s*"([^"]*)"', ',', 'string'),
            ('from', '"from"\\s*:\\s*"([^"]*)"', ',', 'string'),
            ('sys_thinking_value', '"sys_thinking_value"\\s*:\\s*"(.*?)"(?=\\s*,\\s*"role_thinking_value")', ',', 'string'),
            ('role_thinking_value', '"role_thinking_value"\\s*:\\s*"(.*?)"(?=\\s*})', '}', 'string')
        ]
        
        # ä¸åŒæ¨¡å‹å¯ä»¥æœ‰ä¸åŒçš„æ¨¡å¼
        model_patterns = {
            'claude_3_7_sonnet_20250219': json_patterns,  # ä½¿ç”¨æ–°çš„JSONæ¨¡å¼
            'claude_opus_4_20250514': json_patterns,
            'claude_sonnet_4_20250514': json_patterns,
            'gemini_2_5_pro': json_patterns,
            'gemini_2_5_pro_preview_05_06': json_patterns,
            'gemini_2_5_pro_preview_06_05': json_patterns,
            'default': json_patterns
        }
        
        return model_patterns.get(model_name, json_patterns)
    
    field_patterns = get_field_patterns(model_name)
    extract_data = []
    success_count = 0
    error_count = 0
    
    for idx, item in enumerate(all_processed_data):
        try:
            item['enhanced_result'] = []
            response_text = item.get('response', '')
            
            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿response_textæ˜¯å­—ç¬¦ä¸²
            if not isinstance(response_text, str):
                response_text = str(response_text) if response_text else ''
            
            if idx % 100 == 0:  # æ¯100æ¡æ‰“å°ä¸€æ¬¡è¿›åº¦
                print(f"Processing item {idx+1}/{len(all_processed_data)}, response length: {len(response_text)}")
            
            # é¦–å…ˆå°è¯•æå–JSONæ ¼å¼çš„æ•°æ®
            json_items = extract_json_response(response_text)
            
            if json_items:
                # å¦‚æœæˆåŠŸæå–åˆ°JSONæ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
                item['enhanced_result'] = json_items
                extract_data.append(item)
                success_count += 1
                
                if idx % 100 == 0:
                    print(f"æˆåŠŸæå–JSONæ•°æ®ï¼ŒåŒ…å« {len(json_items)} ä¸ªé¡¹ç›®")
            else:
                # å¦‚æœæ²¡æœ‰JSONæ•°æ®ï¼Œä½¿ç”¨åŸæœ‰çš„å­—æ®µæå–æ–¹æ³•ä½œä¸ºå¤‡ç”¨
                result_dict = {
                    'scenario': "",
                    'topic': "",
                    'key_characters': []
                }
                
                for field_name, start_pattern, end_pattern, field_type in field_patterns:
                    try:
                        field_value = extract_field(response_text, start_pattern, end_pattern)
                        
                        # æ ¹æ®å­—æ®µç±»å‹è¿›è¡Œåå¤„ç†
                        if field_type == 'character_array' and field_name == 'key_characters':
                            field_value = parse_characters(field_value)
                        elif field_type == 'string':
                            # æ¸…ç†å­—ç¬¦ä¸²ä¸­çš„è½¬ä¹‰å­—ç¬¦
                            if isinstance(field_value, str):
                                field_value = field_value.replace('\\"', '"').replace('\\n', '\n').replace('\\t', '\t')
                        elif field_type == 'integer':
                            # å¤„ç†æ•´æ•°ç±»å‹
                            try:
                                field_value = int(field_value) if field_value else 0
                            except ValueError:
                                field_value = 0
                        
                        result_dict[field_name] = field_value
                    except Exception as field_error:
                        print(f"Warning: Error processing field {field_name} in item {idx}: {field_error}")
                        # è®¾ç½®é»˜è®¤å€¼
                        if field_type == 'character_array':
                            result_dict[field_name] = []
                        elif field_type == 'integer':
                            result_dict[field_name] = 0
                        else:
                            result_dict[field_name] = ""
                        continue
                
                item['enhanced_result'].append(result_dict)
                extract_data.append(item)
                success_count += 1
            
        except Exception as e:
            print(f"Error processing item {idx}: {e}")
            error_count += 1
            # å³ä½¿å‡ºé”™ä¹Ÿä¿ç•™åŸå§‹æ•°æ®
            item['enhanced_result'] = [{'error': str(e)}]
            extract_data.append(item)
    
    print(f"æŠ½å–å®Œæˆ: æˆåŠŸ {success_count} æ¡, é”™è¯¯ {error_count} æ¡")
    
    # ç»Ÿè®¡å„å­—æ®µçš„æŠ½å–æƒ…å†µ
    field_stats = {'id': 0, 'type': 0, 'from': 0, 'sys_thinking_value': 0, 'role_thinking_value': 0, 
                   'scenario': 0, 'topic': 0, 'key_characters': 0}
    character_count = 0
    json_items_count = 0
    
    for item in extract_data:
        if item.get('enhanced_result'):
            enhanced_results = item['enhanced_result']
            
            # å¦‚æœæ˜¯JSONæ ¼å¼çš„æ•°æ®ï¼ˆå¤šä¸ªé¡¹ç›®ï¼‰
            if isinstance(enhanced_results, list) and len(enhanced_results) > 1:
                json_items_count += len(enhanced_results)
                for result in enhanced_results:
                    # ç»Ÿè®¡æ–°çš„JSONå­—æ®µ
                    for field in ['id', 'type', 'from', 'sys_thinking_value', 'role_thinking_value']:
                        if result.get(field):
                            field_stats[field] += 1
            
            # å¦‚æœæ˜¯å•ä¸ªç»“æœï¼ˆæ—§æ ¼å¼æˆ–JSONä¸­åªæœ‰ä¸€ä¸ªé¡¹ç›®ï¼‰
            elif isinstance(enhanced_results, list) and len(enhanced_results) >= 1:
                result = enhanced_results[0]
                
                # ç»Ÿè®¡JSONå­—æ®µ
                for field in ['id', 'type', 'from', 'sys_thinking_value', 'role_thinking_value']:
                    if result.get(field):
                        field_stats[field] += 1
                
                # ç»Ÿè®¡æ—§æ ¼å¼å­—æ®µ
                if result.get('scenario'):
                    field_stats['scenario'] += 1
                if result.get('topic'):
                    field_stats['topic'] += 1
                if result.get('key_characters'):
                    field_stats['key_characters'] += 1
                    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿key_charactersæ˜¯åˆ—è¡¨
                    if isinstance(result['key_characters'], list):
                        character_count += len(result['key_characters'])
                    else:
                        print(f"Warning: key_characters is not a list in item: {type(result['key_characters'])}")
    
    print("å­—æ®µæŠ½å–ç»Ÿè®¡:")
    print(f"  JSONé¡¹ç›®æ€»æ•°: {json_items_count}")
    for field, count in field_stats.items():
        if count > 0:  # åªæ˜¾ç¤ºæœ‰æ•°æ®çš„å­—æ®µ
            print(f"  {field}: {count}/{len(extract_data)} ({count/len(extract_data)*100:.1f}%)")
    if character_count > 0:
        print(f"  æ€»è§’è‰²æ•°é‡: {character_count}")
    
    # ä¿å­˜ç»“æœ
    enhanced_output_path = output_path.replace('.jsonl', '_enhanced_result.jsonl')
    with open(enhanced_output_path, 'w', encoding='utf-8') as f:
        for item in extract_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"å¢å¼ºç»“æœå·²ä¿å­˜åˆ°: {enhanced_output_path}")
    
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤„ç†vulcanè¾“å‡ºæ ¼å¼çš„æ•°æ®")
    parser.add_argument("--input_dir", type=str, required=True, help="è¾“å…¥ç›®å½•ï¼ŒåŒ…å«jsonlæ–‡ä»¶")
    parser.add_argument("--model_name", type=str, required=True, help="æ¨¡å‹åç§°")
    args = parser.parse_args()
    print(f"args: {args}")
    process_directory(args.input_dir, args.model_name)




python_cmd = """
python  /path/to/data/example
    --input_dir /path/to/data/example
    --model_name claude_3_7_sonnet_20250219
    """
python_cmd = """
python /path/to/data/example
    --input_dir /path/to/data/example
    --model_name model
    """

python_cmd = """
python /path/to/data/example
    --input_dir /path/to/data/example
    --model_name model
    """
